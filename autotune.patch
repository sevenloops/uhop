*** Begin Patch
*** Add File: uhop/ops_registry.py
+"""
+Simple operation registry for UHOP elementwise/autotune demo.
+Holds canonical op names and metadata used by the autotuner/dispatch.
+"""
+from typing import Callable, Dict, Any
+import json
+from pathlib import Path
+
+_OPS: Dict[str, Dict[str, Any]] = {}
+
+def register_op(name: str, fallback: Callable = None, *, supports_broadcast: bool = True):
+    """Register an op with optional fallback implementation and metadata."""
+    _OPS[name] = {
+        "name": name,
+        "fallback": fallback,
+        "supports_broadcast": supports_broadcast,
+    }
+    return _OPS[name]
+
+def get_op(name: str):
+    return _OPS.get(name)
+
+def list_ops():
+    return list(_OPS.keys())
+
+def save_registry(path: str):
+    p = Path(path)
+    p.write_text(json.dumps({k: {"supports_broadcast": v["supports_broadcast"]} for k,v in _OPS.items()}, indent=2))
+
+# Register default simple ops
+def _default_add(a, b):
+    return a + b
+
+register_op("add", fallback=_default_add, supports_broadcast=True)
+register_op("mul", fallback=lambda a,b: a * b, supports_broadcast=True)
+
*** End Patch
*** Begin Patch
*** Add File: uhop/backends/cupy_wrapper.py
+"""
+Minimal CuPy wrapper for compiling & launching RawModule CUDA kernels.
+This wrapper is intentionally small: it compiles a CUDA kernel string and
+exposes a simple `build_and_launch` helper used by the autotuner.
+"""
+from pathlib import Path
+import time
+import json
+import os
+
+try:
+    import cupy as cp
+except Exception:
+    cp = None  # defensive; callers should check
+
+def ensure_cupy():
+    if cp is None:
+        raise RuntimeError("CuPy is required for the CUDA backend. Install cupy (pip install cupy-cuda11x or cupy).")
+
+class CupyKernel:
+    def __init__(self, source: str, kernel_name: str):
+        ensure_cupy()
+        # RawModule accepts .cu source and compiles with NVCC (by default)
+        self._module = cp.RawModule(code=source, backend='nvcc')
+        self._fn = self._module.get_function(kernel_name)
+
+    def launch(self, grid, block, args, stream=None):
+        """Launch the compiled kernel. args must be cupy arrays or scalars."""
+        if stream is None:
+            self._fn(grid, block, tuple(args))
+        else:
+            self._fn(grid, block, tuple(args), stream=stream)
+
+def time_kernel_run(kernel: CupyKernel, grid, block, args, warmups=3, runs=10):
+    """Time kernel execution using cupy.cuda.get_current_stream() synchronization."""
+    ensure_cupy()
+    stream = cp.cuda.get_current_stream()
+    # Warmup
+    for _ in range(warmups):
+        kernel.launch(grid, block, args, stream=stream)
+    # timed runs
+    stream.synchronize()
+    start = time.perf_counter()
+    for _ in range(runs):
+        kernel.launch(grid, block, args, stream=stream)
+    stream.synchronize()
+    end = time.perf_counter()
+    return (end - start) / runs
+
+def arrays_to_device(*arrays, dtype=None):
+    ensure_cupy()
+    dev = []
+    for a in arrays:
+        if isinstance(a, cp.ndarray):
+            dev.append(a)
+        else:
+            dev.append(cp.array(a, dtype=dtype))
+    return dev
+
+def device_name():
+    ensure_cupy()
+    dev = cp.cuda.runtime.getDevice()
+    name = cp.cuda.runtime.getDeviceProperties(dev).get('name', 'cuda')
+    return name
+
*** End Patch
*** Begin Patch
*** Add File: uhop/autotuner.py
+"""
+Autotuner for elementwise ops (minimal PoC).
+
+Features:
+- Compile candidate kernels (from kernel template files)
+- Benchmark each candidate using cupy backend
+- Store best (kernel_id + launch config) to cache (JSON)
+
+This module is intentionally compact so it can be expanded to other ops.
+"""
+import json
+import os
+from pathlib import Path
+import math
+from typing import Dict, Any, Tuple
+import time
+from jinja2 import Template
+
+from . import ops_registry
+from .backends import cupy_wrapper
+
+CACHE_DIR = Path("uhop/cache")
+KERNELS_DIR = Path("uhop/kernels")
+
+def _ensure_cache_dir():
+    CACHE_DIR.mkdir(parents=True, exist_ok=True)
+
+def _cache_path(device: str, op_name: str) -> Path:
+    return CACHE_DIR / device / f"{op_name}.json"
+
+def _load_cache(device: str, op_name: str):
+    p = _cache_path(device, op_name)
+    if p.exists():
+        try:
+            return json.loads(p.read_text())
+        except Exception:
+            return None
+    return None
+
+def _save_cache(device: str, op_name: str, data: Dict[str, Any]):
+    p = _cache_path(device, op_name)
+    p.parent.mkdir(parents=True, exist_ok=True)
+    p.write_text(json.dumps(data, indent=2))
+
+def _render_kernel_template(path: Path, context: Dict[str, Any]) -> str:
+    text = path.read_text()
+    tpl = Template(text)
+    return tpl.render(**context)
+
+def _default_block_candidates():
+    # typical block sizes to try (one-dimensional kernels)
+    return [128, 256, 512, 1024]
+
+def compile_kernel_from_template(template_path: Path, kernel_name: str, context: Dict[str, Any]):
+    source = _render_kernel_template(template_path, context)
+    return cupy_wrapper.CupyKernel(source, kernel_name)
+
+def autotune_elementwise(op_name: str, size: int, dtype: str = "float32", device: str = "cuda"):
+    """
+    Autotune elementwise op `op_name` for a single flat size.
+    Returns cached best config or runs tuning and caches the result.
+    """
+    if device != "cuda":
+        raise NotImplementedError("This autotuner currently supports the 'cuda' device only.")
+
+    cached = _load_cache(device, op_name)
+    if cached:
+        return cached
+
+    templates_dir = KERNELS_DIR / "cuda"
+    # For now we only have elementwise_add template; map op_name->filename
+    mapping = {
+        "add": "elementwise_add.cu.jinja",
+        "mul": "elementwise_add.cu.jinja",  # same kernel with different op
+    }
+    filename = mapping.get(op_name)
+    if filename is None:
+        raise ValueError(f"No kernel template mapped for op {op_name}")
+
+    template_path = templates_dir / filename
+    if not template_path.exists():
+        raise FileNotFoundError(f"Kernel template {template_path} not found.")
+
+    best = {"latency_s": float("inf")}
+    candidates = _default_block_candidates()
+
+    # Prepare sample arrays
+    import numpy as np
+    a = np.random.rand(size).astype(dtype)
+    b = np.random.rand(size).astype(dtype)
+    import cupy as cp
+    # put arrays on device once per candidate to avoid transfer timing
+    da = cp.asarray(a)
+    db = cp.asarray(b)
+    dout = cp.empty_like(da)
+
+    for block in candidates:
+        threads = block
+        grid = math.ceil(size / threads)
+        context = {
+            "OP_EXPR": "a[i] + b[i]" if op_name == "add" else "a[i] * b[i]",
+            "KERNEL_NAME": "elem_op",
+            "DTYPE": dtype,
+        }
+        try:
+            kernel = compile_kernel_from_template(template_path, "elem_op", context)
+        except Exception as e:
+            # compilation failed; skip candidate
+            continue
+
+        # args: pointers + size (use cupy arrays and Python int)
+        args = (da, db, dout, size)
+        try:
+            latency = cupy_wrapper.time_kernel_run(kernel, (grid,1,1), (threads,1,1), args, warmups=2, runs=6)
+        except Exception:
+            continue
+
+        if latency < best["latency_s"]:
+            best = {
+                "latency_s": latency,
+                "block": threads,
+                "grid": grid,
+                "dtype": dtype,
+                "kernel_source_context": context,
+            }
+
+    if best["latency_s"] == float("inf"):
+        raise RuntimeError("No candidate kernel successfully compiled and timed.")
+
+    _ensure_cache_dir()
+    _save_cache(device, op_name, best)
+    return best
+
+def get_cached_or_tune(op_name: str, size: int, dtype: str = "float32", device: str = "cuda"):
+    return autotune_elementwise(op_name, size=size, dtype=dtype, device=device)
+
*** End Patch
*** Begin Patch
*** Add File: uhop/kernels/cuda/elementwise_add.cu.jinja
+// elementwise kernel template (CUDA)
+#include <stdint.h>
+extern "C" __global__
+void {{ KERNEL_NAME }}(const {{ DTYPE }}* __restrict__ a, const {{ DTYPE }}* __restrict__ b, {{ DTYPE }}* __restrict__ out, size_t N) {
+    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
+    size_t stride = gridDim.x * blockDim.x;
+    for (size_t i = idx; i < N; i += stride) {
+        {{ DTYPE }} aval = a[i];
+        {{ DTYPE }} bval = b[i];
+        out[i] = {{ OP_EXPR }};
+    }
+}
+
*** End Patch
*** Begin Patch
*** Add File: tests/test_elementwise_autotune.py
+import os
+import json
+import pytest
+import numpy as np
+
+from pathlib import Path
+
+try:
+    import cupy as cp
+except Exception:
+    cp = None
+
+from uhop import ops_registry
+from uhop import autotuner
+from uhop.backends import cupy_wrapper
+
+pytestmark = pytest.mark.skipif(cp is None, reason="cupy required to run CUDA autotuner tests")
+
+def test_autotune_add_and_correctness(tmp_path):
+    # ensure cache cleared for test device/op
+    cache_file = Path("uhop/cache") / "cuda" / "add.json"
+    if cache_file.exists():
+        cache_file.unlink()
+
+    size = 1_000_00  # 100k elements
+    best = autotuner.get_cached_or_tune("add", size=size, dtype="float32", device="cuda")
+    assert "latency_s" in best
+    assert best["latency_s"] > 0
+
+    # compile the selected kernel and run one correctness check
+    template_path = Path("uhop/kernels/cuda/elementwise_add.cu.jinja")
+    context = best["kernel_source_context"]
+    src = template_path.read_text()
+    from jinja2 import Template
+    source = Template(src).render(**context)
+    kernel = cupy_wrapper.CupyKernel(source, context.get("KERNEL_NAME", "elem_op"))
+
+    a = np.random.rand(size).astype("float32")
+    b = np.random.rand(size).astype("float32")
+    da = cp.asarray(a)
+    db = cp.asarray(b)
+    dout = cp.empty_like(da)
+
+    block = best["block"]
+    grid = best["grid"]
+    kernel.launch((grid,1,1), (block,1,1), (da, db, dout, size))
+    cp.cuda.get_current_stream().synchronize()
+    got = dout.get()
+    expect = a + b
+    # numeric compare
+    assert np.allclose(got, expect, atol=1e-5, rtol=1e-4)
+
*** End Patch
