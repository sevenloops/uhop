"""
Triton Backend for UHOP Multi-Backend System.

Provides kernel auto-generation and compilation using OpenAI Triton.
Integrates with the unified backend architecture while preserving existing
Triton helper functions.
"""
from typing import Dict, List, Optional, Callable
import logging

from uhop.backends.base import Backend, KernelSource

logger = logging.getLogger(__name__)


# Existing helper function preserved for backward compatibility
def is_triton_available():
    try:
        import triton  # type: ignore  # noqa: F401
        return True
    except Exception:
        return False


class TritonBackend(Backend):
    """
    Triton backend for auto-generated GPU kernels.
    
    Triton is a Python DSL for writing high-performance GPU kernels
    that work on both NVIDIA and AMD GPUs.
    """
    
    def __init__(self):
        super().__init__("triton")
        self._triton = None
        self._torch = None
        self._triton_available = False
    
    def initialize(self) -> bool:
        """Initialize Triton backend."""
        if self._initialized:
            return self.capabilities.available
        
        try:
            import triton
            import triton.language as tl
            import torch
            
            self._triton = triton
            self._torch = torch
            
            # Triton requires CUDA or ROCm
            if not torch.cuda.is_available():
                self.capabilities.available = False
                self.capabilities.error_msg = "No CUDA/ROCm device available"
                self._initialized = True
                return False
            
            self._triton_available = True
            
            # Get device info from PyTorch
            self.capabilities.available = True
            self.capabilities.device_count = torch.cuda.device_count()
            
            for i in range(self.capabilities.device_count):
                device_name = torch.cuda.get_device_name(i)
                self.capabilities.device_names.append(device_name)
                
                props = torch.cuda.get_device_properties(i)
                if hasattr(props, 'major'):  # CUDA
                    self.capabilities.compute_capability = f"{props.major}.{props.minor}"
                else:  # ROCm
                    self.capabilities.compute_capability = getattr(props, 'gcnArchName', 'unknown')
                self.capabilities.memory_gb = props.total_memory / (1024**3)
            
            self.capabilities.vendor_libs["triton"] = True
            
            logger.info(
                f"[Triton] Initialized with {self.capabilities.device_count} device(s)"
            )
            
            # Register autogenerated kernels
            self._setup_triton_kernels()
            
        except ImportError:
            self.capabilities.available = False
            self.capabilities.error_msg = "triton not installed"
            logger.warning("[Triton] triton not available (install with: pip install triton)")
        except Exception as e:
            self.capabilities.available = False
            self.capabilities.error_msg = str(e)
            logger.error(f"[Triton] Initialization failed: {e}")
        
        self._initialized = True
        return self.capabilities.available
    
    def check_vendor_libs(self) -> Dict[str, bool]:
        """Check Triton availability."""
        return {
            "triton": self._triton_available,
        }
    
    def get_supported_ops(self) -> List[str]:
        """Get list of Triton-supported operators."""
        # Triton can theoretically implement any operator
        ops = [
            # Linear algebra
            "matmul", "bmm",
            # Convolutions
            "conv1d", "conv2d",
            # Activations
            "relu", "gelu", "silu", "sigmoid", "tanh",
            # Normalization
            "layernorm", "rmsnorm", "groupnorm",
            # Softmax
            "softmax", "logsoftmax",
            # Elementwise
            "add", "mul", "div",
            # Reductions
            "sum", "mean", "max",
            # Attention
            "scaled_dot_product_attention", "multi_head_attention",
            # Fused ops (Triton's specialty)
            "fused_bias_gelu", "fused_add_layernorm", "fused_matmul_bias_silu",
            # Pooling
            "maxpool2d", "avgpool2d", "adaptiveavgpool2d",
        ]
        return ops
    
    def _synchronize(self):
        """Synchronize device."""
        if self._torch is not None and self._torch.cuda.is_available():
            self._torch.cuda.synchronize()
    
    def _setup_triton_kernels(self):
        """Set up Triton auto-generated kernels."""
        if not self._triton_available or self._triton is None:
            return
        
        import triton
        import triton.language as tl
        
        # Simplified kernels for demonstration
        # In production, these would be more optimized
        
        def triton_relu(x):
            """Triton-based ReLU."""
            if not isinstance(x, self._torch.Tensor):
                x = self._torch.tensor(x, dtype=self._torch.float32)
            x = x.cuda()
            return self._torch.nn.functional.relu(x)  # Fallback to PyTorch for now
        
        self.register_autogen_kernel("relu", triton_relu)
        
        def triton_gelu(x):
            """Triton-based GELU."""
            if not isinstance(x, self._torch.Tensor):
                x = self._torch.tensor(x, dtype=self._torch.float32)
            x = x.cuda()
            return self._torch.nn.functional.gelu(x)  # Fallback to PyTorch for now
        
        self.register_autogen_kernel("gelu", triton_gelu)
        
        logger.debug(f"[Triton] Registered {len(self._autogen_kernels)} autogen kernels")
    
    def compile_kernel_from_template(self, op_name: str, template: str) -> Optional[Callable]:
        """
        Compile a Triton kernel from a template string.
        
        Args:
            op_name: Operator name
            template: Triton kernel code as string
            
        Returns:
            Compiled kernel function or None
        """
        if not self._triton_available:
            return None
        
        try:
            # Compile template
            local_scope = {
                'triton': self._triton,
                'tl': __import__('triton.language'),
                'torch': self._torch,
            }
            exec(template, local_scope)
            
            # Extract kernel function (assumes it's named after op)
            kernel_fn = local_scope.get(f'triton_{op_name}')
            if kernel_fn:
                self.register_autogen_kernel(op_name, kernel_fn)
                logger.info(f"[Triton] Compiled kernel for '{op_name}' from template")
                return kernel_fn
            
        except Exception as e:
            logger.error(f"[Triton] Failed to compile kernel for '{op_name}': {e}")
        
        return None


# Legacy function preserved for backward compatibility


def triton_matmul(*args, **kwargs):
    if not is_triton_available():
        raise RuntimeError("triton not available")
    # Lazy import to avoid linter errors when Triton isn't installed
    import numpy as np  # type: ignore
    import triton  # type: ignore
    import triton.language as tl  # type: ignore

    @triton.jit
    def _kernel(
            A_ptr,
            B_ptr,
            C_ptr,
            M,
            N,
            K,
            sa0,
            sa1,
            sb0,
            sb1,
            sc0,
            sc1,
            BLOCK: tl.constexpr,
    ):
        pid = tl.program_id(0)
        offs_m = pid * BLOCK + tl.arange(0, BLOCK)
        offs_n = tl.arange(0, BLOCK)
        A = tl.load(A_ptr + offs_m[:, None] * sa0 + offs_n[None, :] * sa1)
        B = tl.load(B_ptr + offs_m[:, None] * sb0 + offs_n[None, :] * sb1)
        C = tl.dot(A, B)
        tl.store(C_ptr + offs_m[:, None] * sc0 + offs_n[None, :] * sc1, C)

    a, b = args
    a = np.array(a, dtype=np.float32)
    b = np.array(b, dtype=np.float32)
    M, K = a.shape
    K2, N = b.shape
    assert K == K2
    C = np.zeros((M, N), dtype=np.float32)
    BLOCK = 64
    grid = ((M + BLOCK - 1) // BLOCK,)
    _kernel[(grid,)](
        a,
        b,
        C,
        M,
        N,
        K,
        a.strides[0] // 4,
        a.strides[1] // 4,
        b.strides[0] // 4,
        b.strides[1] // 4,
        C.strides[0] // 4,
        C.strides[1] // 4,
        BLOCK=BLOCK,
    )
    return C


def triton_relu(x):
    if not is_triton_available():
        raise RuntimeError("triton not available")
    import numpy as np  # type: ignore
    import torch  # type: ignore
    t = torch.from_numpy(np.array(x)).cuda()
    return torch.relu(t).cpu().numpy()


def triton_conv2d(*args, **kwargs):
    raise NotImplementedError("triton conv2d helper not implemented here")
