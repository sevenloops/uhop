Metadata-Version: 2.4
Name: uhop
Version: 0.1.0
Summary: Universal Hardware Optimization Protocol (UHOP) — AI-powered runtime kernel optimizer for developers.
Home-page: https://github.com/sevenloops/uhop
Author: UHOP Systems
Author-email: UHOP Systems <danbis664@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/sevenloops/uhop
Project-URL: Documentation, https://uhop.dev/docs
Project-URL: Source, https://github.com/sevenloops/uhop
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.26.0
Requires-Dist: torch>=2.2.0
Requires-Dist: triton>=2.2.0; platform_system == "Linux"
Requires-Dist: openai>=1.44.0
Requires-Dist: py-cpuinfo>=9.0.0
Requires-Dist: psutil>=5.9.8
Requires-Dist: pyopencl>=2024.2
Requires-Dist: tqdm>=4.66.4
Requires-Dist: tabulate>=0.9.0
Requires-Dist: orjson>=3.10.7
Requires-Dist: filelock>=3.15.0
Requires-Dist: rich>=13.7.1
Requires-Dist: coloredlogs>=15.0.1
Requires-Dist: click>=8.1.7
Requires-Dist: typing-extensions>=4.12.0
Requires-Dist: packaging>=24.1
Provides-Extra: dev
Requires-Dist: pytest>=8.3.3; extra == "dev"
Requires-Dist: pytest-benchmark>=4.0.0; extra == "dev"
Requires-Dist: ipython>=8.27.0; extra == "dev"
Requires-Dist: jupyter>=1.1.1; extra == "dev"
Provides-Extra: amd
Requires-Dist: rocm-py>=6.0.1; extra == "amd"
Provides-Extra: nvidia
Requires-Dist: cupy-cuda12x>=13.0.0; extra == "nvidia"
Dynamic: author
Dynamic: home-page
Dynamic: requires-python

# UHop MVP (developer-integrated)

This repository is an MVP for UHOP — a runtime that:

- Detects hardware (CUDA vs CPU),
- Runs hand-written CUDA kernels (if PyCUDA available),
- Generates CUDA kernels via OpenAI (if configured),
- Benchmarks and caches the best implementation,
- Exposes a decorator `@hop.optimize("matmul")` for easy integration.

## Quickstart

1. Clone project.
2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   # or at minimum:
   pip install numpy openai
   # Optional for CUDA:
   pip install pycuda
   ```

## CLI

After installing this package in your environment, you get a `uhop` command:

- `uhop info` — print detected hardware and backend availability (Torch, Triton, OpenCL).
- `uhop info --json` — machine-readable JSON hardware info (includes backend availability).
- `uhop info --ocl-device 0` — override OpenCL GPU device selection by index (across all platforms).
- `uhop demo --size 192` — run a quick Naive Python vs UHOP-optimized matmul benchmark and show which wins.
- `uhop demo --iters 5 --ocl-device 0` — adjust iterations and explicitly choose an OpenCL GPU.

Environment override: set `UHOP_OPENCL_DEVICE_INDEX=<idx>` to select a default OpenCL device for the session.

## extra demos

- MatMul (UHOP vs naive):
  - `uhop demo --size 192`
  - Expect UHOP to win decisively vs naive Python; increase size for more stress.
- Fused Conv2D+ReLU (OpenCL fused kernel):
  - `python -m uhop.cli demo-conv2d-relu --h 128 --w 128 --c-in 3 --c-out 32 --k 3 --stride 1 --padding 1`
  - Use `--ocl-device` to select a GPU, and try larger shapes (e.g. `--h 224 --w 224 --c-in 16 --c-out 32`) for stronger GPU gains.

See `docs/RUN_REPORT.md` for a summary of errors encountered and solutions applied during development, plus sample benchmark outputs.
